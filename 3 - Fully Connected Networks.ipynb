{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Fully Connected Networks\n",
    "\n",
    "### Fully connected networks are the classic starting point of neural networks. This is how word embedings are created. \n",
    "\n",
    "### Fully connected network consists of \n",
    "* Input layer\n",
    "* Several hidden layers\n",
    "* Output layer\n",
    "\n",
    "### Again we will illistrate this classical method using the MNIST dataset. This could jest as easily be radiograph images although a great deal of data prepartation is done for us.\n",
    "\n",
    "\n",
    "<BR>\n",
    "<BR>\n",
    "<img src=\"./images/fully-connected.png\" width=\"400px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time we have to define an activation function for the output of our hidden layer. \n",
    "\n",
    "#### Why do we use the Sigmoid function instead of the step function?\n",
    "\n",
    "<BR>\n",
    "<img align=\"left\" style=\"float: l;\" src=\"./images/sigmoid-function.png\" width=\"350px\">\n",
    "<img style=\"float: l;\" src=\"./images/sig-derivative-graph.png\" width=\"400px\">\n",
    "<img src=\"./images/sigmoid derivative.png\" width=\"300px\">\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This convenience function will randomly initialize our weights  to values from a normal distribution \n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the previous dataset with a new dataset of cases\n",
    "def update_d(prev, new):\n",
    "    combined = prev.copy()\n",
    "    combined.update(new)\n",
    "    return combined \n",
    "\n",
    "# We will define a fnction to train our model and display how it is doing\n",
    "def train_model(sess, train_X, train_Y, test_X, test_Y, train_operation, accuracy_operation, num_epochs, batch_size, test_size):\n",
    "    accuracies = []\n",
    "    train_feed = dict()\n",
    "    test_feed = dict()\n",
    "    startingTime = time.time()\n",
    "    with tqdm.tqdm(total = num_epochs * len(train_X)) as ranger:\n",
    "        for epoch in range(num_epochs): # An epoch is a run through our entire training data set\n",
    "            for start in range(0, len(train_X), batch_size): # loop in batches of batch size\n",
    "                end = start + batch_size\n",
    "                sess.run(train_operation, feed_dict=update_d(train_feed, {X: train_X[start:end],y: train_Y[start:end]}))\n",
    "                ranger.update(batch_size)\n",
    "                if (start // batch_size) % 100 == 0: # Let's record accuracy every batch\n",
    "                    testSet = np.random.choice(len(test_X), test_size, replace=False)\n",
    "                    tstX, tstY = test_X[testSet], test_Y[testSet]\n",
    "                    accuracies.append(sess.run(accuracy_operation, feed_dict = update_d(test_feed, {X: tstX, y: tstY})))\n",
    "                    ranger.set_description(\"Test Accuracy: \" + str(accuracies[-1]))\n",
    "            # print out the final test accuracy                                                       \n",
    "            testSet = np.random.choice(len(test_X),test_size,replace=False)\n",
    "            tstX, tstY = test_X[testSet], test_Y[testSet]\n",
    "            accuracies.append(sess.run(accuracy_operation, feed_dict = update_d(test_feed, {X: tstX, y: tstY})))\n",
    "            ranger.set_description(\"Test Accuracy: \" + str(accuracies[-1]))\n",
    "    timeTaken = time.time() - startingTime\n",
    "    print(\"Finished training for %d epochs\" % num_epochs)\n",
    "    print(\"Took %.02f seconds (%.02f s per epoch)\" % (timeTaken, timeTaken/num_epochs))\n",
    "    accuracies.append(sess.run(accuracy_operation, feed_dict = update_d(test_feed, {X:test_X, y: test_Y})))\n",
    "    print(\"Final accuracy was %.04f\" % accuracies[-1])\n",
    "    plt.plot(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder is not a value! We just need to define how we will hold onto our training cases. \n",
    "# \"None\" means that this dimension can be any length.\n",
    "\n",
    "# X will be our input placeholder for a image case\n",
    "X = tf.placeholder(\"float\",shape=[None,784])\n",
    "# Y will be a placeholder for our output distribution\n",
    "y = tf.placeholder(\"float\",shape=[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define our weight variables coming into the hidden layer and entering the output layer\n",
    "# We take advantage of the convenience function init_weights to do this work.\n",
    "NUM_HIDDEN = 300\n",
    "W_h = init_weights([784,NUM_HIDDEN]) # Weights entering the hidden layer\n",
    "W_o = init_weights([NUM_HIDDEN,10]) # Weights entering the output layer\n",
    "b_h = init_weights([1, NUM_HIDDEN]) # Let's define our bias weights for the hidden layer\n",
    "b_o = init_weights([1, 10]) # Let's define our bias weights for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate our weights between the input layer and the hidden layer\n",
    "entering_hidden = tf.matmul(X,W_h) + b_h\n",
    "# Let's execute our activation function on the hidden layer neurons\n",
    "exiting_hidden = tf.nn.sigmoid(entering_hidden)\n",
    "#Let's calculate our weights between the output of the hidden layer nad the input into our softmax output layer\n",
    "y_hat = tf.matmul(exiting_hidden,W_o) + b_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to train our model\n",
    "\n",
    "#### First we have to understand what it means  for a model to be good! IN machine learning we actually define what it means for a model to be bad. We call this our cost of loss function. Our goal is to minimize our loss.\n",
    "\n",
    "#### A commonly used loss function is \"cross-entropy\". Cross-entropy loss increases as the predicted probability diverges from the actual label. It looks something like this . . .\n",
    "\n",
    "<BR>\n",
    "<img src=\"./images/cross_entropy.png\" width=\"400px\"> \n",
    "<BR>\n",
    "\n",
    "<center>$H_{y'}(y) = - \\displaystyle \\sum_{i} y'_i \\space log(y_i) $</center>\n",
    "\n",
    "#### Where  <i>y</i> is our predicted probability distribution, <i>y'</i> is our true distribution (the one-hot vector with the digit labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When we calcualte our evidence we end up with unnormalized log probabilities such as below . . .\n",
    "\n",
    "$evidence_i = \\displaystyle \\sum_{j=0} p(\\space W_{i,j} \\space x_j \\space + \\space b_i) $\n",
    "\n",
    "#### Let's assume a 3 class problem . . . \n",
    "* Training Case 1 Prediction: [ 0.5,  1.5,  0.1]\n",
    "* Training Case 2 Prediction: [ 2.2,  1.3,  1.7]\n",
    "\n",
    "#### These outputs do not sum to one, that is they are unromalized probabilities\n",
    "\n",
    "#### Now Softmax is going to normalize these into linear probabilites\n",
    "\n",
    "$y_{hat} = softmax(evidence)$\n",
    "\n",
    "* Training Case 1 Softmax: [0.227863, 0.61939586, 0.15274114]\n",
    "* Training Case 2 Softmax: [0.49674623,0.20196195,0.30129182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can define our loss function. Cross entropy with logits and we average across our batch.\n",
    "# We generate unnormalized log probabilities (aka logits) and we want the outputs normalized linear probabilities\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "<BR>\n",
    "<img align=\"left\" style=\"float: l;\" src=\"./images/gd.png\" width=\"400px\">\n",
    "\n",
    "<img style=\"float: l;\" src=\"./images/gd-learning.png\" width=\"350px\">\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our gradient descent optimizer\n",
    "# We want to minimize our loss function, that is cross entropy. We will set a learning rate of 0.5\n",
    "train_operation = tf.train.GradientDescentOptimizer(0.2).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Let's read in our MNIST data\n",
    "mnist = input_data.read_data_sets(\"data\", one_hot=True)\n",
    "trX, trY = mnist.train.images, mnist.train.labels\n",
    "tstX, tstY = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.98:  89%|██████████████████████████████████████████▌     | 2441150/2750000 [02:16<00:18, 17139.64it/s]"
     ]
    }
   ],
   "source": [
    "# Let's define our prediction operation, that is the largest probability among our 10 possible output labels\n",
    "predict_operation = tf.argmax(y_hat, 1)\n",
    "# Let's define our accuracy operation\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(tf.equal(predict_operation,tf.argmax(y,1)),tf.float32))\n",
    " \n",
    "NUM_EPOCHS = 50 # Number of complte time through our traing data\n",
    "BATCH_SIZE = 50 # Training batches\n",
    "accuracies = []\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    train_model(sess, trX, trY, tstX, tstY, train_operation, accuracy_operation, NUM_EPOCHS, BATCH_SIZE, 10000)\n",
    "plt.ylim(.8,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many hidden neurons do I really need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
