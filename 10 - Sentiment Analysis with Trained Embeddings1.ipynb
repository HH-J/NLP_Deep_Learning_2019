{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Define some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in our Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'data/movie_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's prepare our training examples\n",
    "\n",
    "### Note that we pass \"True\" to process_docs() so that we only return the training cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 1800\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "positive_lines = process_docs('data/txt_sentoken/pos', vocab, True)\n",
    "negative_lines = process_docs('data/txt_sentoken/neg', vocab, True)\n",
    "sentences = negative_lines + positive_lines\n",
    "print('Total training sentences: %d' % len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit tokenizer to our document set\n",
    "\n",
    "### We will use the tokenizer to sequence our documents based on a word index of our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0215f4eb6ba9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# fit the tokenizer on the documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_docs' is not defined"
     ]
    }
   ],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Encoding\n",
    "\n",
    "### We will first encode our text with our vocabulary word index we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 16, 899, 3475, 46, 2103, 690, 3381, 1284, 14, 1164, 2, 377, 1149, 612, 1439, 18, 24, 4105, 445, 522, 136, 3, 9981, 94, 2000, 13894, 3, 899, 1297, 2068, 553, 256, 1454, 27, 3686, 56, 349, 5, 3476, 2, 1053, 92, 1175, 9143, 9, 409, 748, 6618, 754, 285, 222, 3563, 7910, 6, 27, 619, 149, 2484, 9, 1517, 137, 19605, 2, 6619, 138, 492, 159, 3382, 1133, 2756, 2273, 355, 3, 20, 233, 226, 199, 7005, 311, 1092, 19606, 1251, 59, 72, 990, 256, 445, 71, 914, 13, 402, 52, 273, 325, 79, 4, 273, 604, 13895, 16192, 815, 34, 4564, 1265, 77, 482, 199, 1872, 2608, 39, 218, 150, 9144, 1, 60, 147, 2683, 14, 152, 4251, 9, 776, 226, 476, 182, 61, 662, 3029, 51, 154, 3029, 200, 253, 419, 122, 21, 77, 326, 2883, 5, 1640, 5168, 22, 1176, 107, 4401, 5402, 1455, 4, 78, 3564, 1922, 113, 11016, 329, 21, 23, 151, 142, 50, 137, 21, 1, 326, 493, 1611, 229, 30, 4, 143, 21, 177, 72, 5, 173, 662, 19607, 1736, 59, 2069, 497, 838, 7006, 9145, 308, 114, 3935, 122, 350, 3, 1587, 3192, 924, 14, 26, 2806, 39, 38, 22, 320, 18, 564, 45, 195, 34, 2807, 2136, 11016, 71, 3, 544, 785, 731, 1, 114, 47, 13896, 203, 7911, 159, 749, 899, 13894, 3, 948, 493, 2485, 1150, 1492, 184, 334, 23, 1360, 10, 21, 142, 105, 159, 6, 107, 89, 2274, 9146, 536, 275, 1923, 15, 157, 949, 33, 4948, 776, 4949, 46, 9145, 1391, 350, 214, 1, 78, 509, 13, 19608, 770, 1, 37, 1849, 37, 2221, 1298, 1151, 19609, 498, 159, 7007, 9147, 279, 159, 553, 220, 1737, 13897, 588, 558, 19, 205, 899, 1392, 637, 16193, 79, 19, 248, 544, 6620, 415, 50, 1119, 290, 12, 3280, 367, 16, 58, 580, 1393, 6621, 101, 92, 870, 2808, 3936, 3937, 402, 2001, 6268, 883, 1234, 1235, 2486, 2486, 6622, 222, 3563, 7910, 325, 5958, 4950]\n"
     ]
    }
   ],
   "source": [
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "print(encoded_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Prepare our Training Set\n",
    "\n",
    "### We will also pad our sentences to a fixed size based on the maximium length document in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Prepare our Test Set\n",
    "\n",
    "### Note that we pass \"False\" to process_docs() so that we only return the test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all test reviews\n",
    "positive_docs = process_docs('data/txt_sentoken/pos', vocab, False)\n",
    "negative_docs = process_docs('data/txt_sentoken/neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Size\n",
    "\n",
    "### Why are we adding 1 to our vocabulary size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1317, 100)         2576800   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1310, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 655, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20960)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                209610    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,812,053\n",
      "Trainable params: 2,812,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "# The Sequential model is a linear stack of layers.\n",
    "model = Sequential()\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than (vocabulary size).\n",
    "# now model.output_shape == (None, 1317, 100), where None is the batch dimension.\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "# sentences are linear lists of words, so conv1d with a window size of 8 and 32 filters\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "# Downsample our input - helps with over-fitting\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# Flatten out our dimensions\n",
    "model.add(Flatten())\n",
    "# Feed into a fully connected layer\n",
    "model.add(Dense(10, activation='relu'))\n",
    "# Feed into a final neuron\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train the model\n",
    "\n",
    "### Adam Optimizer: Computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 5s - loss: 0.6894 - acc: 0.5400\n",
      "Epoch 2/10\n",
      " - 3s - loss: 0.5613 - acc: 0.7217\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.1152 - acc: 0.9617\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.0079 - acc: 0.9994\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 3s - loss: 9.5839e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 3s - loss: 7.3775e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 3s - loss: 5.9259e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 3s - loss: 4.8831e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xe927a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Test on our Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.000000\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
